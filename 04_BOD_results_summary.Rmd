---
title: "BOD results and visualization"
author: "Lauren Kremer"
date: "2024-06-14"
output: html_document
---

# Biological Oxygen Demand assays - data exploration

**Objectives**:<br>
1. Format and summarize data from Biological Oxygen Demand (BOD) assays conducted on stream and well samples from the Fraser Experimental Forest. The samples were collected from 2021 to 2022, and the experiments were conducted in the summer of 2024.<br>
2. Visualize the distributions and identify outliers in the BOD data.<br>
3. Explore relationships between BOD data and Excitation-Emission Matrices (EEMs), as well as BOD and topographical/landcover characteristics of the subwatershed for each sample.
```{r install-packages2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pkgTest <- function(x)
{
  if (x %in% rownames(installed.packages()) == FALSE) {
    install.packages(x, dependencies= TRUE)
  }
  library(x, character.only = TRUE)
}

# Make a vector of the packages you need
neededPackages <- c('dplyr', 'tidyverse', 'lubridate', 'ggplot2','ggpubr', 'plotly', 'gridExtra', 'zoo', 'patchwork') #tools for plot titles 

# For every package in the vector, apply your pkgTest function
for (package in neededPackages){pkgTest(package)}
```


<!-- Load Data:
One of the purposes in this chunk is to figure out the adjustment period of each sample before DO depletion can be estimated. Depending on the temp of the individual samples and incubator, this can vary among samples but usually DO is stabilized after 24 hours. This finds the peak value within the first 24 hours and uses that as the start point for each sample.-->

```{r load-data-frames2, include=FALSE}
# List all CSV files in the directory
filelist <- list.files(paste(getwd(), '/datafiles/test_doc', sep = ''), pattern = "\\.csv$", full.names = TRUE)

# Initialize an empty list to store data frames
data_list <- list()

# Loop through each file
for (file in filelist) {
  # Read the CSV file
  df <- read.csv(file)
  # Extract parts of the filename
  filename_parts <- strsplit(basename(file), "_")[[1]]
  # Extract the desired information from the filename parts
  df$innoculate <- filename_parts[3]  # Assuming this is the correct part
  df$sample_no <- as.factor(filename_parts[4]) 
  df$sensor <- as.factor(filename_parts[2]) 
  df$exper_no <- as.factor(filename_parts[1]) 
  df$chamberno <- as.integer(gsub("\\.csv$", "", filename_parts[5])) # Remove the file extension if present
  df$datetime <- mdy_hm(df$Date, tz = "MST", quiet = TRUE)
  
  # Calculate the total number of days each sample was measured
  total_run_days <- as.numeric(difftime(max(df$datetime), min(df$datetime), units = "days"))
  # Add the total.run.days column to the dataframe
  df$total.run.days <- total_run_days
  
  # Here we are dropping early [DO] measurements collected during the sample adjustment period, usually between 12 and 36 hrs
  # Filter data between the 6th and 36 hours (360 to 1440 minutes)
  df_24h <- df %>% filter(between(Delta.T..min., 720, 2160))
  #df_24h <- df %>% filter(Delta.T..min. <= 1440)
  # Check for the peak Oxygen value in the first 24 hours
  peak_oxygen <- max(df_24h$Oxygen, na.rm = TRUE)
  # Find the time (Delta.T..min.) when the peak oxygen occurs
  peak_time <- df_24h %>%
    filter(Oxygen == peak_oxygen) %>%
    pull(Delta.T..min.) %>% 
    min()  # In case there are multiple peaks, use the earliest time

  df <- df %>%
  filter(Delta.T..min. >= peak_time)
    
  # Ensure oxygen_600 is a single value
  if (length(peak_oxygen) == 1) {
    # Calculate the difference from 8.43 calulated 90% sat
    diff_from_8.43 <- peak_oxygen - 8.43
    
    # Adjust all Oxygen values by this difference
    df <- df %>%
      mutate(Oxygen = Oxygen - diff_from_8.43)
  }
  
    # Calculate the 2-hour rolling average (12 rows for 2 hours with 10-min intervals)
  df <- df %>%
    arrange(datetime) %>%  # Make sure data is sorted by time
    mutate(Oxygen_roll_avg = rollapply(Oxygen, width = 36, FUN = mean, fill = NA, align = 'right'))
  
    # Add the data frame to the list
  data_list[[length(data_list) + 1]] <- df
}
# Combine all data frames into one data frame
resp_df <- do.call(rbind, data_list)

# Now data_list contains all the data frames with the additional columns for watershed and year
# Drop unwanted columns from resp_df
resp_df <- resp_df %>% 
  dplyr::select(-Calibration, -User, -Temperature.Unit, -Salinity..pmil., -User.Signal.Intensity, -Error, -Error.Code, -Amplitude..µV., -Reference.Amplitude..µV., -Signal.LED.Current, -Reference.LED.Current, -Battery..V.)
  
```

<!-- Let's add DOC information from the 4 stages:
Stream DOC
resuspended DOC
microcosm start DOC
microcosm end DOC -->

```{r load-moredata-frames2, include=FALSE}
file_path <- file.path(getwd(), "datafiles/BOD_log.csv")

# generate new column names
new_names <- c("test.group", "Box.no.", "sensorno", "microcosm.number", "prep_bottle", 'sample_no', 'site', 'collection.date', 'original.DOC.mg_L',  'resuspended.DOC.read.date', 'resuspended.DOC.mg_L', 'resuspended.TN', 'pre_run.DOC.read.date', 'pre_run.DOC.mg_L', 'test.period.days', 'exp.start.date', 'post_run.DOC.read.date', 'post_run.DOC.mg_L', 'status', 'notes') 

# Adjust the names as needed
DOC_df <- read.csv(file_path) %>%
  rename_with(~ new_names, everything()) %>%
  mutate(sample_no = as.factor(sample_no)) %>%
  mutate(change_DOC = pre_run.DOC.mg_L-post_run.DOC.mg_L )

# And merge based on sample no. 
doc_resp <- merge(DOC_df, resp_df, by = 'sample_no', all.y = TRUE)
```

```{r compare_dataframes2, include=FALSE}
# Find values in doc_resp$sample_no that are not in DOC_df$sample_no
diff_doc_resp <- setdiff(unique(doc_resp$sample_no), unique(DOC_df$sample_no))

# Find values in DOC_df$sample_no that are not in doc_resp$sample_no
diff_DOC_resp <- setdiff(unique(DOC_df$sample_no), unique(doc_resp$sample_no))

# Print the differences
cat("Values in doc_resp not in DOC_df:\n")
print(diff_doc_resp)

cat("Values in DOC_df not in doc_resp:\n")
print(diff_DOC_resp)
```


```{r oxdeplete_raw_all2, include=FALSE}
# Ensure doc_resp$datetime is a proper datetime object
doc_resp$datetime <- as.POSIXct(doc_resp$datetime, format = "%m/%d/%Y %H:%M:%S", tz = "MST")
# Filter the dataframe
doc_resp <- doc_resp %>%
  filter(datetime > as.POSIXct("2024-07-02 12:00:00", tz = "MST"))


ggplot(doc_resp, aes(x = Delta.T..min., y = Oxygen_roll_avg)) +
  geom_point() +             # To add points
  geom_line() +              # To add lines connecting the points
  facet_wrap(~ sample_no) +  # To create a unique plot for each sample_no
  labs(x = "Delta T (min)",  # Label for x-axis
       y = "Oxygen",         # Label for y-axis
       title = "Oxygen vs Delta T for Each Sample") +  # Title of the plot
  theme_minimal()            # Minimal theme for clean look
```

```{r oxdeplete_raw_single, include=FALSE}
plotsubset <- doc_resp[doc_resp$sample_no == '317',]
ggplot(plotsubset, aes(x = Delta.T..min., y = Oxygen_roll_avg)) +
  geom_point() +             # To add points
  geom_line() +              # To add lines connecting the points
  facet_wrap(~ sample_no) +  # To create a unique plot for each sample_no
  labs(x = "Delta T (min)",  # Label for x-axis
       y = "Oxygen",         # Label for y-axis
       title = "Oxygen vs Delta T for Each Sample") +  # Title of the plot
  theme_minimal()            # Minimal theme for clean look
```

<!--Format merged data: So experiments over different day can be plotted together, we will make a column that indicates the time elapsed during the experiment: -->

```{r df_arrange, include=FALSE}
doc_resp <- doc_resp %>%
  group_by(sample_no) %>%
  arrange(datetime) %>%
  mutate(time_elapsed = as.numeric(difftime(datetime, min(datetime), units = "mins")))  # Change units as needed
```

<!--Calculate daily rate of change-->
```{r calculations, include=FALSE}
# Function to calculate the daily rate of change for each spot_no
calculate_daily_rate_of_change <- function(data) {
  # Convert datetime to numeric values representing minutes since start
  data$minutes_since_start <- as.numeric(difftime(data$datetime, min(data$datetime), units = "mins"))
  
  # Fit a linear model
  lm_fit <- lm(Oxygen_roll_avg ~ minutes_since_start, data = data)
  
  # Get the slope (rate of change per minute)
  slope_per_minute <- coef(lm_fit)[2]
  
  # Convert slope to daily rate of change
  slope_per_day <- unname(slope_per_minute * 1440) # 1440 minutes in a day
  
  return(slope_per_day)
}

# Apply the function to each spot_no
rates_of_change <- doc_resp %>%
  group_by(sample_no) %>%
  dplyr::summarize(daily_rate_of_change_Ox = calculate_daily_rate_of_change(cur_data()),
            mean_Oxygen = mean(Oxygen)) %>%
  ungroup() %>%
  left_join(dplyr::select(doc_resp, sample_no, site, pre_run.DOC.mg_L, original.DOC.mg_L, change_DOC, total.run.days), by = "sample_no") %>%
  distinct()

#print(rates_of_change)

# add change_DOC/test.period.days as a column
rates_of_change$daily_DOC_loss <- rates_of_change$change_DOC/rates_of_change$total.run.days
```
<!--Format daily rate of change-->
```{r format_daily_rate_of_change, include=FALSE}
rates_of_change <- rates_of_change %>%
  mutate(watershed = as.factor(str_extract(site, "^[^_]+"))) %>%
  mutate(watershed = if_else(watershed == 'dhs', 'dh', watershed))

rates_of_change <- rates_of_change %>%
  mutate(type = case_when(
    grepl("dumm", sample_no) ~ "dummy",
    grepl("inn", sample_no) ~ "innoculate",
    grepl("H2O", sample_no) ~ "H2O",
    TRUE ~ "sample"
  ))
```
<!--These structures are used for plotting later-->
```{r}
stat_subset <- rates_of_change %>%
  filter(type == 'sample', watershed != 'SSFP') %>%
  dplyr::select(-watershed) %>%
  mutate(ID = as.integer(substr(as.character(sample_no), 1, 3)))
  
```

```{r import_eems_topo, echo=FALSE, warning=FALSE, message=FALSE}
#eems_discharge_canopy.csv

file_path <- file.path(getwd(), "datafiles/fluoro_topo_melt_PARAFAC.csv")
other_data <- read.csv(file_path)%>%
  mutate(datetime = ymd_hms(datetime))%>%
  dplyr::select(!c('site'))
```

```{r merge_to_BODdf, echo=FALSE, warning=FALSE, message=FALSE}
# Merge dataframes
# Merge by multiple columns
merged_df <- merge(stat_subset, other_data, by = "ID") %>%
  filter(sample_no != 335)
  #mutate(date = mdy(date))

```


**Workflow:**<br>
In the script, three datasets were imported:<br>
1. the DO depletion at 10min intervals as recorded by PreSens instrument as experimental microcosms <br>
2. the pre- and post-experiment DOC measurements for each sample as measured by the Shimadzu<br>
3. EEMs results for the corresponding sample (if EEMs were collected for that sample) as well as topography, landcover data for the respective subwatershed. <br>

Formatting steps for the BOD data include:<br>
1. DO depletion:<br>
  - All tests/samples required an incubator adjustment period after the initiation of the test. [DO] would start low and climb as microcosms cooled and settled. Therefore, the peak DO between the 12th and 36th hour for each sample was used as the test 'start'. <br>
  - All 10min intervals were converted to a 2 hr rolling average to smooth short-term variation.
  -[DO] measurements were then transformed to a change in [DO] for each day by finding the slope of [DO]/time.<br>
2. Change in DOC (Non-purgeable organic carbon or NPOC)<br>
  - Experimental difference was determined by subtracting post-experimental reading from pre-experiment, and 

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Figure. What is the relationship between measured change in sample DOC and daily DO loss? The relationship appears to be linear, but with a lot of variability."}
# Create the plot with points and regression line
plot <- ggplot(rates_of_change %>% drop_na(change_DOC), 
               aes(x = daily_DOC_loss, y = daily_rate_of_change_Ox)) +
    geom_point() +  # Add points to the plot
    geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Add regression line
    labs(x = "Daily change in DOC (mg/L)", y = "Daily Rate of DO loss (mg/L)", title = "Daily change in DOC vs Daily rate of DO loss (mg/L)") +
  stat_cor(method = "pearson", label.x.npc = "left", label.y.npc = "top", size = 4, na.rm = TRUE) +
    theme_minimal()

# Display the plot
print(plot)
```


```{r generate_predicted_rate, include=FALSE}
# Fit the linear model
model <- lm(daily_rate_of_change_Ox ~ I(daily_DOC_loss), data = rates_of_change)

# Generate the predicted values using the model
rates_of_change$predicted_daily_rate <- predict(model, newdata = rates_of_change)
```

Let's look at the distribution of each of these datasets and identify variability and outliers.<br>
We have two measurements from these experiments: the O2 within the microcosm measured at 10min intervals, and the change in DOC from the beginning of the experiment to the end. <br>

## Daily DOC loss (carbon) 
which represents the change in DOC standardized by the length of the experiment, calculated from the DOC measurement at the beginning of the experiment minus the DOC measurement after the experiment, divided by the number of days in the experiment.<br>

Let's check the distribution and outliers of the change in DOC dataset. 
Visualize the distribution with a boxplot:<br>
```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(merged_df, aes(y = daily_DOC_loss)) +
  geom_boxplot(fill = "lightgreen", color = "black") +
  labs(title = "Boxplot of Daily DOC Loss", y = "Daily DOC Loss") +
  theme_minimal()
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
Q1 <- quantile(merged_df$daily_DOC_loss, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_df$daily_DOC_loss, 0.75, na.rm = TRUE)
IQR_value <- IQR(merged_df$daily_DOC_loss, na.rm = TRUE)

# Define outlier thresholds
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Identify outliers
outliers <- merged_df %>%
  filter(daily_DOC_loss < lower_bound | daily_DOC_loss > upper_bound)

# View the outliers
outliers$daily_DOC_loss
```

It looks like we have 2 outliers that do note make sense (daily_DOC_loss), where the 'loss' is negative (<0.1), implying that DOC increased post-experiment. Let's remove those outliers and look at the distribution of DOC loss among watersheds.
```{r, echo=FALSE, warning=FALSE, message=FALSE}

# Remove only the two largest outliers from the dataset
no_bod_outliers<- merged_df %>%
  filter(!sample_no %in% c(282, 354, 323))

# Remove only the two largest outliers from the dataset
ratesforexport <- rates_of_change %>%
  mutate(ID = as.integer(substr(as.character(sample_no), 1, 3))) %>%
  filter(!sample_no %in% c(282, 354, 323))
  
  
write.csv(ratesforexport, 'datafiles/BODrates_cleaned.csv', row.names = FALSE)
```

```{r boxplotDOC, echo=FALSE, warning=FALSE, message=FALSE, fig.cap = "Figure. A boxplot of the measured DOC change in a sample/by # of experiment days across watersheds."}

# Create the box plot
box_plot <- ggplot(no_bod_outliers, aes(x = watershed, y = daily_DOC_loss)) +
  geom_boxplot() +
  labs(title = "Comparison of Experiment DOC Change Across Watersheds",
       x = "Watershed",
       y = "Daily DOC change") +
  theme_minimal()

# Display the plot
box_plot
```

## Dissolved oxygen loss
Let's check the distribution and outliers of the dissolved oxygen loss data. 
Visualize the distribution with a boxplot:<br>

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ggplot(merged_df, aes(y = daily_rate_of_change_Ox)) +
  geom_boxplot(fill = "lightblue", color = "black") +
  labs(title = "Boxplot of Daily DO Loss", y = "Daily DO Loss") +
  theme_minimal()
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
Q1 <- quantile(merged_df$daily_rate_of_change_Ox, 0.25, na.rm = TRUE)
Q3 <- quantile(merged_df$daily_rate_of_change_Ox, 0.75, na.rm = TRUE)
IQR_value <- IQR(merged_df$daily_rate_of_change_Ox, na.rm = TRUE)

# Define outlier thresholds
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

# Identify outliers
outliers <- merged_df %>%
  filter(daily_rate_of_change_Ox < lower_bound | daily_rate_of_change_Ox > upper_bound)

# View the outliers
#outliers
```

No outliers here that I am concerned about, so lets look at distribution across watersheds:

```{r boxplotDO, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Figure. A boxplot of the measured daily rate of DO change across watersheds. This measurement seems to be much variable within watershed"}
# Create the box plot
box_plot <- ggplot(no_bod_outliers, aes(x = watershed, y = daily_rate_of_change_Ox)) +
  geom_boxplot() +
  labs(title = "Comparison of Daily Rate of oxygen Change Across Watersheds",
       x = "Watershed",
       y = "Daily Rate of Change") +
  theme_minimal()

# Display the plot
box_plot
```

## Explore relationships.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
correlation_df <- no_bod_outliers %>%
  dplyr::select(daily_rate_of_change_Ox, daily_DOC_loss, percent_1.5, percent_6.10, percent_11.15, percent_.15, ndvi_raster_mean, ndvi_raster_median, percent_Beetle.kill, percent_Meadows, percent_Old.Forest, percent_cut.regenerating, percent_Tundra )

# Scale the data
scaled_df <- correlation_df %>%
  mutate(across(everything(), scale))

# Calculate correlation and p-value matrices on the scaled data
Mp <- rcorr(as.matrix(scaled_df))

# Rename the labels in the correlation matrix
colnames(Mp$r) <- c("daily_rate_of_change_Ox", "daily_DOC_loss", "% canopy 1 to 5m", "% canopy 6 to 10m", "% canopy 11 to 15m", "% canopy > 15m", "NDVI Mean", "NDVI Median", "% Beetle Kill", "% Meadows", "% Old Forest",  "% Cut/Regenerating", "% Tundra")

rownames(Mp$r) <- c("daily_rate_of_change_Ox", "daily_DOC_loss", "% canopy 1 to 5m", "% canopy 6 to 10m", "% canopy 11 to 15m", "% canopy > 15m", "NDVI Mean", "NDVI Median", "% Beetle Kill", "% Meadows", "% Old Forest",  "% Cut/Regenerating", "% Tundra")

# Plot matrix
corrplot(Mp$r[1:2, 3:13], 
         p.mat=Mp$P[1:2, 3:13], 
         method="color", 
         col = COL2('PuOr', 10),
         sig.level = c(0.001, 0.01, 0.05), 
         tl.col = 'black', 
         tl.srt = 45,
         insig = 'label_sig',
         pch.cex = 1)
```



```{r, echo=FALSE, warning=FALSE, message=FALSE}
correlation_df <- no_bod_outliers %>%
  dplyr::select(daily_rate_of_change_Ox, daily_DOC_loss, peak_A_250.450, peak_C_350.450, peak_T_275.340, Fluorescence_Index, SUVA254, peak_ratioAT, peak_ratioCA, peak_ratioCT)

# Scale the data
scaled_df <- correlation_df %>%
  mutate(across(everything(), scale))

# Calculate correlation and p-value matrices on the scaled data
Mp <- rcorr(as.matrix(scaled_df))

# Rename the labels in the correlation matrix
colnames(Mp$r) <- c("DO daily rate of change", "daily DOC loss", "peak_A_250.450", "peak_C_350.450", "peak_T_275.340", "Fluorescence_Index", "SUVA254", "peak_ratioAT", "peak_ratioCA", "peak_ratioCT")

rownames(Mp$r) <- c("DO daily rate of change", "daily DOC loss", "peak_A_250.450", "peak_C_350.450", "peak_T_275.340","Fluorescence_Index", "SUVA254", "peak_ratioAT", "peak_ratioCA", "peak_ratioCT")

# Plot matrix
corrplot(Mp$r[1:2, 3:10], 
         p.mat=Mp$P[1:2, 3:10], 
         method="color", 
         col = COL2('PuOr', 10),
         sig.level = c(0.001, 0.01, 0.05), 
         tl.col = 'black', 
         tl.srt = 45,
         insig = 'label_sig',
         pch.cex = 1)
```


```{r, echo=FALSE, warning=FALSE, message=FALSE}
correlation_df <- no_bod_outliers %>%
  dplyr::select(daily_rate_of_change_Ox, daily_DOC_loss, area_km, Qm3_s, ndmi_raster_mean, ndmi_raster_median, twi_mean, twi_median, slope_mean, slope_median,  disttostreams_mean, aspect_mean, resampled_mean, distance.to.outlet)

# Scale the data
scaled_df <- correlation_df %>%
  mutate(across(everything(), scale))

# Calculate correlation and p-value matrices on the scaled data
Mp <- rcorr(as.matrix(scaled_df))

# Rename the labels in the correlation matrix
colnames(Mp$r) <- c('daily_rate_of_change_Ox', 'daily_DOC_loss', 'area (km)', 'Q (m^3/s)', 'NDMI mean', 'NDMI median', 'TWI mean', 'TWI median', 'slope mean', 'slope median', 'dist to streams mean', 'aspect mean', 'elevation mean', 'distance to outlet')

rownames(Mp$r) <- c('daily_rate_of_change_Ox', 'daily_DOC_loss', 'area (km)', 'Q (m^3/s)', 'NDMI mean', 'NDMI median', 'TWI mean', 'TWI median', 'slope mean', 'slope median', 'dist to streams mean', 'aspect mean', 'elevation mean', 'distance to outlet')

# Plot matrix
corrplot(Mp$r[1:2, 3:10], 
         p.mat=Mp$P[1:2, 3:10], 
         method="color", 
         col = COL2('PuOr', 10),
         sig.level = c(0.001, 0.01, 0.05), 
         tl.col = 'black', 
         tl.srt = 45,
         insig = 'label_sig',
         pch.cex = 1)
```



```{r, echo=FALSE, warning=FALSE, message=FALSE}
correlation_df <- no_bod_outliers %>%
  dplyr::select(daily_rate_of_change_Ox, daily_DOC_loss, PARAF_comp_1,           PARAF_comp_2, PARAF_comp_3, total_loadings, relative_load_comp_1, relative_load_comp_2, relative_load_comp_3, norm_PARAF_c1, norm_PARAF_c2, norm_PARAF_c3)

# Scale the data
scaled_df <- correlation_df %>%
  mutate(across(everything(), scale))

# Calculate correlation and p-value matrices on the scaled data
Mp <- rcorr(as.matrix(scaled_df))

# Rename the labels in the correlation matrix
colnames(Mp$r) <- c('daily_rate_of_change_Ox', 'daily_DOC_loss', 'comp 1 absorbance (humic)', 'comp 2 absorbance (humic)', 'comp 3 absorbance (protein)', 'total_loadings', 'comp 1 fraction (humic)', 'comp 2 fraction (humic)', 'comp 3 fraction (protein)', 'DOC normalized comp1', 'DOC normalized comp2', 'DOC normalized comp3')

rownames(Mp$r) <- c('daily_rate_of_change_Ox', 'daily_DOC_loss', 'comp 1 absorbance (humic)', 'comp 2 absorbance (humic)', 'comp 3 absorbance (protein)', 'total_loadings', 'comp 1 fraction (humic)', 'comp 2 fraction (humic)', 'comp 3 fraction (protein)', 'DOC normalized comp1', 'DOC normalized comp2', 'DOC normalized comp3')

# Plot matrix
corrplot(Mp$r[1:2, 3:12], 
         p.mat=Mp$P[1:2, 3:12], 
         method="color", 
         col = COL2('PuOr', 10),
         sig.level = c(0.001, 0.01, 0.05), 
         tl.col = 'black', 
         tl.srt = 45,
         insig = 'label_sig',
         pch.cex = 1)
```


Filtered datasets can be played with here. If we filter dates to a single synoptic period so that we only have on one point for every landcover classification, the trends look very similar to that of the full dataset, where there is a weak but significant negative correlation between daily DOC change and areas with short canopy (percent_1.5). However, if we filter the entire dataset by sample position (i.e. streams vs wells) there is no correlation for any canopy class if looking at stream samples only. It appears that well samples are driving trends and significance in percent_1.5 and percent6.10. 
```{r, echo=FALSE, warning=FALSE, message=FALSE}
filtered_df <- merged_df %>%
    filter(datetime >= ymd_hms("2022-06-15 00:10:00") & datetime <= as.Date("2022-07-01 00:10:00"))%>%
    #filter(position == 'stream') %>%
    #filter(sample_no != 407) %>%
    #filter(position %in% c('lw1', 'lw2', 'rw1', 'rw2'))%>%
    mutate(watershed = as.factor(watershed))

# List of columns to plot on the x-axis
x_columns <- c("percent_1.5", "percent_6.10", "percent_11.15","percent_.15")

# Create an empty list to store the plots
plot_list <- list()

# Loop through the x_columns to generate a plot for each one
for (x_col in x_columns) {
  min_x <- min(filtered_df[[x_col]], na.rm = TRUE)
  max_y <- max(filtered_df[['daily_rate_of_change_Ox']], na.rm = TRUE)
  # Create a plot with x_col as the x-axis
  p <- ggplot(filtered_df, aes_string(x = x_col, y = 'daily_rate_of_change_Ox', color = 'watershed')) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +  # Add regression line
    stat_cor(aes(group = 1), method = "pearson", label.x = min_x, label.y = max_y, size = 4) +  # Add Pearson correlation
    labs(x = x_col, y = 'Daily DO Change', title = paste( x_col)) +
    theme_minimal()
  
  # Store the plot in the list
  plot_list[[x_col]] <- p
}

# Combine the plots into a 2x2 panel layout using patchwork
combined_plot <- ( plot_list[['percent_1.5']] | plot_list[['percent_6.10']]) / (plot_list[['percent_11.15']] | plot_list[['percent_.15']] )

# Display the combined plot in a 2x2 layout
combined_plot
```
